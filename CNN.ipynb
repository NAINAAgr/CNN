{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnEdddMEi7F4"
      },
      "source": [
        "âœ… QUESTION 1\n",
        "Role of Filters and Feature Maps in CNN\n",
        "\n",
        " ANSWER:\n",
        "\n",
        "Filters (also called kernels) are small learnable matrices that slide over the input image and extract important local features such as edges, corners, textures, shapes, etc.\n",
        "Each filter detects a specific pattern.\n",
        "\n",
        "Feature Maps are the output of convolution operations. They represent the spatial activation of a filter over the input image, showing where the detected feature exists.\n",
        "\n",
        " Filters â†’ detect features\n",
        " Feature Maps â†’ store detected features\n",
        " Multiple filters â†’ learn multiple feature types"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZrMtMsvjOzD"
      },
      "source": [
        "âœ… QUESTION 2 Padding & Stride in CNN and their effect on output size\n",
        "\n",
        "ANSWER:\n",
        "\n",
        "| Term        | Meaning                                              | Effect                                                                  |\n",
        "| ----------- | ---------------------------------------------------- | ----------------------------------------------------------------------- |\n",
        "| **Padding** | Adding extra pixels around the image border          | Helps preserve spatial size, prevents shrinking, improves edge learning |\n",
        "| **Stride**  | Number of pixels the filter jumps during convolution | Larger stride â†’ smaller output size and faster computation              |\n",
        "\n",
        "Output Dimension Formula:\n",
        "\n",
        "Output Size=((ğ‘Šâˆ’ğ¹+2ğ‘ƒ)/ğ‘†)+1\n",
        "\n",
        "\n",
        "Where:\n",
        "W = input size, F = filter size, P = padding, S = stride\n",
        "\n",
        "âœ” Higher padding â†’ larger Output\n",
        "âœ” Higher stride â†’ smaller Output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvGxF9Ztj8l1"
      },
      "source": [
        "âœ… QUESTION 3 Receptive Field & Importance\n",
        "\n",
        "ANSWER:\n",
        "\n",
        "Receptive field = the region of input image that affects a particular neuron in deeper layers.\n",
        "\n",
        "As layers increase in depth, each neuron covers a larger area of the original image.\n",
        "\n",
        " Importance in deep networks:\n",
        "\n",
        "Helps capture global context\n",
        "\n",
        "Detects high-level features (object shapes, faces)\n",
        "\n",
        "Improves classification accuracy for complex images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rixC_70dkIAA"
      },
      "source": [
        "âœ… QUESTION 4 Effect of Filter Size & Stride on Parameters\n",
        "\n",
        "ANSWER:\n",
        "\n",
        "Filter size directly affects number of trainable parameters\n",
        "Parameters = (Filter_width Ã— Filter_height Ã— Input_channels + bias)\n",
        "Larger filters â†’ more parameters â†’ more computational cost\n",
        "\n",
        "Stride does NOT affect parameters\n",
        "It only affects feature map size (downsampling)\n",
        "\n",
        "Smaller filters reduce parameters and help scale to deep networks\n",
        " Stride controls how much information is retained"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKtr-UAYkZ1D"
      },
      "source": [
        "âœ… QUESTION 5 Comparison: LeNet vs AlexNet vs VGG\n",
        "\n",
        "ANSWER:\n",
        "\n",
        "| Architecture           | Depth                    | Filter Size     | Performance                       | Dataset Used |\n",
        "| ---------------------- | ------------------------ | --------------- | --------------------------------- | ------------ |\n",
        "| **LeNet-5 (1998)**     | Small (7 layers)         | 5Ã—5             | Good for digits                   | MNIST        |\n",
        "| **AlexNet (2012)**     | Mediumâ€“Deep (8 layers)   | 11Ã—11, 5Ã—5, 3Ã—3 | Introduced ReLU, GPU training     | ImageNet     |\n",
        "| **VGG16/VGG19 (2014)** | Very Deep (16â€“19 layers) | Only 3Ã—3        | High accuracy with uniform design | ImageNet     |\n",
        "\n",
        "Trend: Increasing depth â†’ better performance\n",
        "VGG introduced design simplicity (stacked 3Ã—3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 6: Keras CNN on MNIST â€“ Code\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Load dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "\n",
        "# Build model\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n",
        "    layers.MaxPooling2D(2,2),\n",
        "    layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D(2,2),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train + evaluate\n",
        "model.fit(x_train, y_train, epochs=5, validation_split=0.1)\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(\"Test Accuracy:\", test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AmUiaSbincq",
        "outputId": "f026f40e-30c4-450b-d9b8-b4cb9d0cf06e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 28ms/step - accuracy: 0.9125 - loss: 0.2981 - val_accuracy: 0.9875 - val_loss: 0.0442\n",
            "Epoch 2/5\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 27ms/step - accuracy: 0.9851 - loss: 0.0477 - val_accuracy: 0.9895 - val_loss: 0.0403\n",
            "Epoch 3/5\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 28ms/step - accuracy: 0.9909 - loss: 0.0287 - val_accuracy: 0.9907 - val_loss: 0.0364\n",
            "Epoch 4/5\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 28ms/step - accuracy: 0.9944 - loss: 0.0188 - val_accuracy: 0.9883 - val_loss: 0.0442\n",
            "Epoch 5/5\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 28ms/step - accuracy: 0.9955 - loss: 0.0138 - val_accuracy: 0.9895 - val_loss: 0.0375\n",
            "\u001b[1m313/313\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.9846 - loss: 0.0439\n",
            "Test Accuracy: 0.9887999892234802\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "id": "gew6nNWnl5w0",
        "outputId": "2832eaed-6e69-406c-9439-9131ab7c80ce"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,272</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ],
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m32\u001b[0m)     â”‚           \u001b[38;5;34m896\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m32\u001b[0m)     â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m64\u001b[0m)     â”‚        \u001b[38;5;34m18,496\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m64\u001b[0m)       â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)      â”‚        \u001b[38;5;34m73,856\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚       \u001b[38;5;34m262,272\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             â”‚         \u001b[38;5;34m1,290\u001b[0m â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">356,810</span> (1.36 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m356,810\u001b[0m (1.36 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">356,810</span> (1.36 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m356,810\u001b[0m (1.36 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "782/782 - 65s - 83ms/step - accuracy: 0.4495 - loss: 1.5114\n",
            "Epoch 2/10\n",
            "782/782 - 63s - 81ms/step - accuracy: 0.5987 - loss: 1.1341\n",
            "Epoch 3/10\n",
            "782/782 - 63s - 81ms/step - accuracy: 0.6605 - loss: 0.9680\n",
            "Epoch 4/10\n",
            "782/782 - 63s - 81ms/step - accuracy: 0.6982 - loss: 0.8549\n",
            "Epoch 5/10\n",
            "782/782 - 63s - 80ms/step - accuracy: 0.7323 - loss: 0.7659\n",
            "Epoch 6/10\n",
            "782/782 - 61s - 79ms/step - accuracy: 0.7585 - loss: 0.6919\n",
            "Epoch 7/10\n",
            "782/782 - 83s - 106ms/step - accuracy: 0.7798 - loss: 0.6336\n",
            "Epoch 8/10\n",
            "782/782 - 62s - 79ms/step - accuracy: 0.8015 - loss: 0.5706\n",
            "Epoch 9/10\n",
            "782/782 - 82s - 105ms/step - accuracy: 0.8178 - loss: 0.5188\n",
            "Epoch 10/10\n",
            "782/782 - 82s - 105ms/step - accuracy: 0.8358 - loss: 0.4634\n",
            "\u001b[1m313/313\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.7281 - loss: 0.8521\n",
            "Test accuracy: 0.7254999876022339\n"
          ]
        }
      ],
      "source": [
        "#âœ… QUESTION 7 CNN for CIFAR-10 Classification (Keras)\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Load data\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train, x_test = x_train/255.0, x_test/255.0\n",
        "\n",
        "# CNN Architecture\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Conv2D(64,(3,3),activation='relu'),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Conv2D(128,(3,3),activation='relu'),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128,activation='relu'),\n",
        "    layers.Dense(10,activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=64, verbose=2)\n",
        "print(\"Test accuracy:\", model.evaluate(x_test, y_test)[1])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 8: PyTorch CNN for MNIST\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Data\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_data = datasets.MNIST(root='data', train=True, transform=transform, download=True)\n",
        "test_data = datasets.MNIST(root='data', train=False, transform=transform, download=True)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=64)\n",
        "\n",
        "# Model\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 3), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, 3), nn.ReLU(), nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(64*5*5, 128), nn.ReLU(),\n",
        "            nn.Linear(128, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = x.view(-1, 64*5*5)\n",
        "        return self.fc(x)\n",
        "\n",
        "model = CNN().to(device) # Move model to GPU if available\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Training\n",
        "print(\"Starting training...\")\n",
        "for epoch in range(5):\n",
        "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device) # Move data to GPU\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (batch_idx + 1) % 100 == 0: # Print loss every 100 batches\n",
        "            print(f'Epoch [{epoch+1}/5], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
        "print(\"Training complete.\")\n",
        "\n",
        "# Evaluation\n",
        "print(\"Evaluating model...\")\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device) # Move data to GPU\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_JYxV9ukcFY",
        "outputId": "92ce5bb6-cc1c-4133-bc91-fb054641f803"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Starting training...\n",
            "Epoch [1/5], Step [100/938], Loss: 0.5698\n",
            "Epoch [1/5], Step [200/938], Loss: 0.2403\n",
            "Epoch [1/5], Step [300/938], Loss: 0.1041\n",
            "Epoch [1/5], Step [400/938], Loss: 0.0697\n",
            "Epoch [1/5], Step [500/938], Loss: 0.1835\n",
            "Epoch [1/5], Step [600/938], Loss: 0.2204\n",
            "Epoch [1/5], Step [700/938], Loss: 0.0962\n",
            "Epoch [1/5], Step [800/938], Loss: 0.0227\n",
            "Epoch [1/5], Step [900/938], Loss: 0.1352\n",
            "Epoch [2/5], Step [100/938], Loss: 0.0979\n",
            "Epoch [2/5], Step [200/938], Loss: 0.0354\n",
            "Epoch [2/5], Step [300/938], Loss: 0.2695\n",
            "Epoch [2/5], Step [400/938], Loss: 0.1277\n",
            "Epoch [2/5], Step [500/938], Loss: 0.0366\n",
            "Epoch [2/5], Step [600/938], Loss: 0.0592\n",
            "Epoch [2/5], Step [700/938], Loss: 0.0128\n",
            "Epoch [2/5], Step [800/938], Loss: 0.0623\n",
            "Epoch [2/5], Step [900/938], Loss: 0.0660\n",
            "Epoch [3/5], Step [100/938], Loss: 0.0651\n",
            "Epoch [3/5], Step [200/938], Loss: 0.0238\n",
            "Epoch [3/5], Step [300/938], Loss: 0.0533\n",
            "Epoch [3/5], Step [400/938], Loss: 0.0187\n",
            "Epoch [3/5], Step [500/938], Loss: 0.0499\n",
            "Epoch [3/5], Step [600/938], Loss: 0.1624\n",
            "Epoch [3/5], Step [700/938], Loss: 0.0341\n",
            "Epoch [3/5], Step [800/938], Loss: 0.0408\n",
            "Epoch [3/5], Step [900/938], Loss: 0.0358\n",
            "Epoch [4/5], Step [100/938], Loss: 0.0010\n",
            "Epoch [4/5], Step [200/938], Loss: 0.0365\n",
            "Epoch [4/5], Step [300/938], Loss: 0.0256\n",
            "Epoch [4/5], Step [400/938], Loss: 0.0211\n",
            "Epoch [4/5], Step [500/938], Loss: 0.0145\n",
            "Epoch [4/5], Step [600/938], Loss: 0.0408\n",
            "Epoch [4/5], Step [700/938], Loss: 0.0361\n",
            "Epoch [4/5], Step [800/938], Loss: 0.0234\n",
            "Epoch [4/5], Step [900/938], Loss: 0.0442\n",
            "Epoch [5/5], Step [100/938], Loss: 0.0022\n",
            "Epoch [5/5], Step [200/938], Loss: 0.0054\n",
            "Epoch [5/5], Step [300/938], Loss: 0.0611\n",
            "Epoch [5/5], Step [400/938], Loss: 0.0023\n",
            "Epoch [5/5], Step [500/938], Loss: 0.0711\n",
            "Epoch [5/5], Step [600/938], Loss: 0.0067\n",
            "Epoch [5/5], Step [700/938], Loss: 0.0076\n",
            "Epoch [5/5], Step [800/938], Loss: 0.0045\n",
            "Epoch [5/5], Step [900/938], Loss: 0.1084\n",
            "Training complete.\n",
            "Evaluating model...\n",
            "Test Accuracy: 98.97%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 9: Keras ImageDataGenerator for Custom Dataset\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import models, layers\n",
        "\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_gen = datagen.flow_from_directory(\n",
        "    'dataset/train', target_size=(128,128),\n",
        "    batch_size=32, class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_gen = datagen.flow_from_directory(\n",
        "    'dataset/val', target_size=(128,128),\n",
        "    batch_size=32, class_mode='categorical'\n",
        ")\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32,(3,3),activation='relu',input_shape=(128,128,3)),\n",
        "    layers.MaxPooling2D(2,2),\n",
        "    layers.Conv2D(64,(3,3),activation='relu'),\n",
        "    layers.MaxPooling2D(2,2),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128,activation='relu'),\n",
        "    layers.Dense(train_gen.num_classes,activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(train_gen, validation_data=val_gen, epochs=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "th8DEmh-nwPG",
        "outputId": "6e79ad0c-08ef-4142-ae9b-cf9aacc8fcfe"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 10 images belonging to 2 classes.\n",
            "Found 4 images belonging to 2 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.5000 - loss: 0.7019 - val_accuracy: 0.5000 - val_loss: 5.9189\n",
            "Epoch 2/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 437ms/step - accuracy: 0.5000 - loss: 5.3232 - val_accuracy: 0.5000 - val_loss: 2.6294\n",
            "Epoch 3/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 361ms/step - accuracy: 0.5000 - loss: 2.3858 - val_accuracy: 0.5000 - val_loss: 2.0017\n",
            "Epoch 4/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 332ms/step - accuracy: 0.5000 - loss: 1.5641 - val_accuracy: 0.5000 - val_loss: 0.7012\n",
            "Epoch 5/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 484ms/step - accuracy: 1.0000 - loss: 0.3239 - val_accuracy: 0.5000 - val_loss: 1.2769\n",
            "Epoch 6/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 752ms/step - accuracy: 0.5000 - loss: 0.8210 - val_accuracy: 0.5000 - val_loss: 1.0890\n",
            "Epoch 7/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 639ms/step - accuracy: 0.5000 - loss: 0.6462 - val_accuracy: 0.5000 - val_loss: 0.8540\n",
            "Epoch 8/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 988ms/step - accuracy: 0.5000 - loss: 0.4318 - val_accuracy: 0.5000 - val_loss: 0.6863\n",
            "Epoch 9/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 368ms/step - accuracy: 1.0000 - loss: 0.2933 - val_accuracy: 0.5000 - val_loss: 0.7404\n",
            "Epoch 10/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 336ms/step - accuracy: 1.0000 - loss: 0.3741 - val_accuracy: 0.5000 - val_loss: 0.6995\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a8e5350e870>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2885bf70",
        "outputId": "67a8c27b-270a-4a64-dd22-461b68005f13"
      },
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Define the base directory for the dummy dataset\n",
        "dataset_base_path = 'dataset'\n",
        "\n",
        "# Define class names and create directories\n",
        "class_names = ['cat', 'dog'] # Example classes\n",
        "train_dir = os.path.join(dataset_base_path, 'train')\n",
        "val_dir = os.path.join(dataset_base_path, 'val')\n",
        "\n",
        "# Create base dataset directory if it doesn't exist\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(val_dir, exist_ok=True)\n",
        "\n",
        "# Create subdirectories for each class within train and val\n",
        "for class_name in class_names:\n",
        "    os.makedirs(os.path.join(train_dir, class_name), exist_ok=True)\n",
        "    os.makedirs(os.path.join(val_dir, class_name), exist_ok=True)\n",
        "\n",
        "# Function to create a dummy image\n",
        "def create_dummy_image(path, filename, size=(128, 128)):\n",
        "    image = Image.fromarray(np.random.randint(0, 255, (size[0], size[1], 3), dtype=np.uint8))\n",
        "    image.save(os.path.join(path, filename))\n",
        "\n",
        "# Create dummy images for 'train' directory\n",
        "print('Creating dummy training images...')\n",
        "for class_name in class_names:\n",
        "    for i in range(5): # Create 5 dummy images per class for training\n",
        "        create_dummy_image(os.path.join(train_dir, class_name), f'{class_name}_{i}.png')\n",
        "\n",
        "# Create dummy images for 'val' directory\n",
        "print('Creating dummy validation images...')\n",
        "for class_name in class_names:\n",
        "    for i in range(2): # Create 2 dummy images per class for validation\n",
        "        create_dummy_image(os.path.join(val_dir, class_name), f'{class_name}_{i}.png')\n",
        "\n",
        "print('Dummy dataset created successfully!')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating dummy training images...\n",
            "Creating dummy validation images...\n",
            "Dummy dataset created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: End-to-end Plan â€“ X-ray Classification (Normal vs Pneumonia)\n",
        "\n",
        "ANSWER:\n",
        "\n",
        "Step-By-Step Approach\n",
        "\n",
        "* Dataset Preparation\n",
        "\n",
        "->Collect labeled chest X-ray images (e.g., Kaggle dataset).\n",
        "\n",
        "->Preprocess: resize (e.g., 224Ã—224), normalize pixel values.\n",
        "\n",
        "->Train-validation-test split (e.g., 70â€“20â€“10).\n",
        "\n",
        "* Model Development\n",
        "\n",
        "->Use a CNN (e.g., EfficientNet or VGG16 transfer learning)\n",
        "\n",
        "->Binary classification â†’ Sigmoid activation + Binary Crossentropy\n",
        "\n",
        "* Training\n",
        "\n",
        "->Use augmentation to reduce overfitting\n",
        "\n",
        "->Monitor accuracy, precision, recall, F1-score\n",
        "\n",
        "* Saving the Model\n",
        "\n",
        "model.save(\"xray_model.h5\")\n",
        "\n",
        "\n",
        "* Deployment using Streamlit\n",
        "\n",
        "->Build UI for image upload\n",
        "\n",
        "->Load model and predict\n",
        "\n",
        "->Show probability + class label\n",
        "\n",
        "* Streamlit Code Example\n",
        "\n",
        "import streamlit as st\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "model = tf.keras.models.load_model(\"xray_model.h5\")\n",
        "\n",
        "st.title(\"Chest X-Ray Classification\")\n",
        "file = st.file_uploader(\"Upload X-Ray Image\")\n",
        "\n",
        "if file:\n",
        "    img = Image.open(file).resize((224,224))\n",
        "    x = np.expand_dims(np.array(img)/255.0, axis=0)\n",
        "    pred = model.predict(x)[0][0]\n",
        "    label = \"Pneumonia\" if pred > 0.5 else \"Normal\"\n",
        "    st.write(f\"Prediction: **{label}**\")\n",
        "\n",
        "\n",
        "* Hosting\n",
        "\n",
        "->Deploy using Streamlit Cloud, Heroku, or AWS\n",
        "\n",
        "->Maintain version updates and model improvements"
      ],
      "metadata": {
        "id": "IYw3VSTipD49"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}